# AI_text_detection_KO

Check out `old_detector` to see more details.

The RoBERTa AI text detector that is only trained on the GPT2 output dataset can be found [here](https://huggingface.co/weizhou03/roberta-old-AI-detector)
The current best RoBERTa AI text detector checkpoint (trained on both GPT2 output dataset and TuringBench dataset) can be found [here](https://drive.google.com/file/d/1nhLOxHZhNOoFhVy06icKT4mjws8a8ODC/view?usp=sharing)

## Progress made for Progress Report 2
- Add `prepare_model_and_data.sh` script to download our current AI text classifier trained on texts generated by human in WebText and by varying configurations of GPT2.
- Add `run_old_eval.sh` script to automatically measure the accuracy of our classifier on the test set of the GPT2 Output Dataset.
- Add `old_detector/eval.py` to separate the training logic and evaluation logic.
- Add the `TuringBenchDataset` class in `old_detector/dataset.py` which prepares the data from TuringBench dataset for future training.

## Progress made for Progress Report 2
- Update `old_detector/train.py` to support finetuning on the TuringBench dataset
- Add `run_old_train.sh` script to automatically finetune the model on the TuringBench dataset
- Add `train.ipynb` which contains the execution results of finetunning the model on the TuringBench dataset
- Add `new_detector/generate_data.py` to generate synthetic datasets from newer LLMs like GPT-4o-mini
